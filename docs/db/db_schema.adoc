= Database schema (high-level design)

WARNING: The database schema in this document is deprecated and will not be kept up-to-date.
To get an up-to-date ER diagram you can deploy the link:../../dao/src/main/resources/db.migration[migrations] to a database and use a tool like https://www.pgadmin.org/docs/pgadmin4/latest/erd_tool.html[this].

This document introduces the database schema of the ORT server.
Since the data to be stored is rather complex, the whole schema is broken down into multiple logical areas.
For each area, there are one or more diagrams showing the entities and their relations.

Note that the diagrams do not model every single detail.
The intended goal is rather to have an overview over the different entities, their most important attributes, and how they are connected.

== General design

This section discusses some general design topics and how they are dealt with in the ORT server schema.

=== Synthetic primary keys

The debate whether to prefer natural or synthetic (artificial/surrogate) primary keys is still ongoing.
See for instance https://sqlstudies.com/2016/08/29/natural-vs-artificial-primary-keys/ or https://medium.com/swlh/relational-databases-know-your-primary-keys-3897befe9d2. This design uses synthetic keys exclusively for the following reasons:

* For many entities - especially in the ORT result model - there are no obvious natural keys.
* There are many relations between tables.
Basing these on synthetic keys is easier and more efficient than using natural keys for this purpose.

=== "Shared" versus "redundant" tables

In typical relational database modeling, a goal is to be free of redundancy as far as possible.
This means that tables define _UNIQUE_ constraints on identifying properties, and a specific instance of an entity exists only once.
This has obvious advantages.
For instance, the size of the database is reduced as it is free of redundancy, or statistics about entities are easier to calculate because no de-duplication has to be implemented.
Therefore, this approach is certainly preferred.
In some constellations, however, there are drawbacks:

* Adding new entities becomes harder.
It has to be checked first whether a specific entity already exists in the table; if so, the ID of this entity has to be retrieved, otherwise, a new row is added to the table.
Postgres offers an https://www.postgresqltutorial.com/postgresql-tutorial/postgresql-upsert/[INSERT ON CONFLICT] or _UPSERT_ statement for this purpose.
However, its usage is not trivial.
* For entities with many attributes it is not always easy to find out whether a specific instance already exists.
This is especially true for `Package` and `Project`: For these entities, identity is usually controlled by the components of an ORT `Identifier` (type, namespace, name, and version).
However, for two entities with the same identifier, it cannot be guaranteed that all other properties match.
For instance, after adding new curations, metadata of a package may change compared to an instance from an older analysis run.
Detecting such cases and preventing redundancy correctly would mean a high effort.

So, choosing one of these approaches exclusively, seems to be too strict; a choice can be made based on the entity type.
For entities with a limited number of attributes (which mainly remain static over time), instances should be de-duplicated and shared.
Examples for this category are (source code) repositories, SPDX license expressions, or author names.
In other cases, as for the mentioned projects and packages - but also for issues, vulnerabilities, or environment information -, new instances can be created for every analysis run.

=== JSON columns

The database schema defined here tries to be rather normalized.
For some cases, an exception was made though, and ORT structures are stored in their serialized form in JSON columns.
This is done mainly for configuration information for the different ORT components, which has the following properties:

* The structures are quite complex consisting of multiple subcomponents including arbitrary key-value pairs.
* The data is mainly stored for reference purpose.
It is not further evaluated during an ORT analysis run.

So, there is no actual benefit in taking the effort to create a relational model for these structures.

== Data model

This section discusses the single areas of the data model of the ORT server.
Each area has its own subsection which - depending on its complexity - may be further broken down.

=== ORT result

In the CLI version of ORT, YAML files are used to store the results of the single processing steps.
This section describes a number of entities that are capable to store equivalent information.
ORT result files have a hierarchical structure, sometimes containing lists of objects within other lists.
This design does not fit that well to relational database structures; therefore, numerous foreign-key relations and association tables are required to store and correctly link the corresponding data.
This makes this part of the database schema quite complicated.
To keep the size of the diagrams manageable, multiple diagrams are created that focus on the single substructures (such as analyzer, scanner, advisor) existing in an ORT result file.

==== Global and shared entities

This diagram contains the entities that do not or not exclusively belong to a specific ORT component.

[plantuml,result_shared,svg]
----
include::db_result_shared.puml[]
----

`ORT_RUN` represents the result of an ORT analysis as a whole and thus corresponds to a single result file.
It can be seen as the entrypoint into the model: starting from here, all information related to an analysis of a repository can be obtained.

The entities `ORT_ISSUE`, `LICENSE_SPDX`, `ENVIRONENT`, and `VCS_REPOSITORY` are used by multiple other components.

==== Analyzer

The analyzer section of an ORT result lists the projects that have been analyzed (i.e. the actual source code) and the external packages they depend on.
This includes the whole dependency graph.

[plantuml,analyzer,svg]
----
include::db_analyzer.puml[]
----

This part of the data model is by far the most complex one.
Entry point is the `ANALYZER_RESULT` entity, which combines the properties of the ORT model classes `AnalyzerRun` and `AnalyzerResult`.
It is associated with the tables for projects and curated packages.
The dependency graph is modelled in a way similar to the structure in an ORT result file; as lists of nodes and edges, and entry points into the graph (for the direct dependencies of project scopes).
In contrast to ORT result files, no numeric indices are used here, but references between entities are expressed via foreign key relations.
(Note that since the dependency graph can contain both packages and projects, these relations can have both tables as targets.)

==== Advisor

The advisor part adds information about vulnerabilities and defects to an ORT result.

[plantuml,advisor,svg]
----
include::db_advisor.puml[]
----

==== Scanner

This part of the model defines license and copyright findings provided by external scanner tools.
Corresponding to the state of the ORT data model, there is no dedicated modelling for snipped scanners; this may be extended in the future.

[plantuml,scanner,svg]
----
include::db_scanner.puml[]
----

==== Evaluator

The last part of an ORT result to be stored in the database is the evaluator result.
Compared to the other parts, this is a quite simple model, since it only adds a set of rule violations.

[plantuml,evaluator,svg]
----
include::db_evaluator.puml[]
----

=== Customer model

This part of the data model deals with information about customers and their repositories they want to analyze with ORT.
This information is organized in a hierarchical model:

On the highest level, there are _organizations_.
Organizations develop _products_.
Products in turn can have multiple _repositories_ containing the actual source code to be analyzed.
On each level, users with different roles can be assigned.

==== Association with Keycloak

There is obviously a strong connection between the entities in this part of the model and the role and access management to be implemented in the server.
For each level of the hierarchy, special roles must exist:

* An admin of an organization can
** add or remove users to or from this organization
** assign roles to users in this organization
** create, update, or delete products in this organization
** create, update, or delete repositories in the products belonging to this organization
* An admin of a product can
** add or remove users to or from this product
** assign repository roles to users in this product
** create, update, or delete repositories in the products belonging to this organization
* An admin of a repository can
** trigger an analysis run on a repository
** assign repository roles to users in this repository
* A user assigned to a repository can
** read the analysis results generated for this repository

For each request sent by a user, the server has to check whether the user has corresponding access rights.
The most convenient way to do this is if the access token provided in the request already contains sufficient information required for this check: the organizations, products, and repositories assigned to the user and all the roles he or she is given.
So, this implies that this information is available in Keycloak.

The data structures supported by Keycloak are actually capable to represent the customer data model: Organizations and products can be mapped to groups (which can be hierarchical), and access rights on repositories can be represented by roles (this approach is currently used for the mini server).
The predefined properties of these structures in Keycloak are quite limited; it is, however, possible to assign arbitrary key-value pairs - so called _attributes_ to them.
So it would be an option to store this data directly in Keycloak.
Updates can be done via the Admin REST API.
If these API calls are hidden behind a repository implementation, for the client code it is fully transparent that the data comes from a different source.

Based on these thoughts, a diagram for the entities in this area could look as follows:

[plantuml,customer,svg]
----
include::db_customer.puml[]
----

==== Credentials

One reoccurring source of problems for ORT analysis runs is the management of credentials.
During an analysis, multiple external systems need to be accessed, e.g. to clone source code or to fetch artifacts, which all require their specific authentication.
Often, development teams use their own methods to deal with credentials that require a special setup of the development environment, such as setting environment variables or using local configuration files.
It is obvious that project-specific conventions like that can be a major challenge for a generic service like ORT.
So it is no wonder that many support requests are related to the topic of credential management.

Traditionally, credentials have been managed centrally, but this approach does not scale and has a number of problems:

* Every time a customer needs access to a new system, the corresponding credentials have to be added by 1st or 2nd-level support.
* There can be (unresolvable) conflicts if different customers require different credentials for accessing the same systems.
* From a security point of view it is not ideal if always all credentials are available in the build environment, although most of them are not required for a specific run.

To address these problems, an ORT server must provide sufficient flexibility that allows project teams to define their specific build environment in regard to the credentials in use.
The responsibility to define correct credentials must be shifted to the project teams, making the life of support easier.
This implies that customers can manage their own credentials and have the ability to configure their usage when analysing a specific repository.
The basic idea is as follows:

The ORT server stores and allows the management of credentials.
There can still be central credentials managed by administrators (for well-known systems frequently used by multiple customers), but each customer can define their own credentials and assign them to their organizational structures (organizations, products, repositories).

Having customer-specific credentials is only halfway to flexible build environments.
There needs to be a way to declare how these credentials are made available during an ORT analysis run.
For instance, a repository built with Maven may require a specific credential to be included as a server password in a `settings.xml` file.
If NPM is used as package manager, special entries in the `.npmrc` file may be required.
Other teams may have the convention that specific environment variables are referenced from their build definition files.
This basically means, that there must be a rather flexible mechanism to map credentials to specific build configuration options.

NOTE: This flexibility comes of course with a price.
It won't be trivial for customers to get their definitions right.
Probably some UI and tooling support will be needed here.

Based on this discussion, credentials could be stored in database tables as shown in the following ER diagram:

[plantuml,credentials,svg]
----
include::db_credentials.puml[]
----

The main entry point into these structures is the `CREDENTIAL` entity.
It represents a single secret or piece of sensitive information, such as a username or a password.
It is given a human-readable name and a description.
The actual secret is stored in the `value` property.
(To improve security, it may be required to manually encrypt this property.) The entity can be assigned to customer structures on different levels: to all the repositories and products of a customer organization, to all the repositories of a customer product, or only to a specific repository.
This association can be expressed by the different nullable foreign key attributes.
By leaving all keys to *null*, the credential becomes a central one (available to all customers).

The remaining entities allow mapping a specific credential to different build configurations.
They support the same set of foreign keys, so that the mapping can again be done on different levels.

* `MAVEN_CREDENTIAL_MAPPING` specifies that a credential should be included (either as username or password) in the `settings.xml` file under a specific server alias name.
* `NPM_REGISTRY` and `NPMRC_CREDENTIAL_MAPPING` control the content of the `.npmrc` configuration file.
Using these entities, customers can define their own scoped NPM registries and the entries to be added to the `.npmrc` file for them.
* `ENV_CREDENTIAL_MAPPING` states that the value of a credential should be available as an environment variable during the execution of an ORT analysis run under a specific name.

If required, more structures could be added to support different types of package manager-specific build configuration files.
Before running an ORT step, the server would read the mapping information for the current repository and generate the build configuration accordingly.

=== Orchestrator

The _orchestrator_ is the central component in the ORT server that divides the work to be done for a single analysis run into multiple jobs, so that it can be executed with a high degree of parallelism.

Traditionally, ORT has been a command line tool.
The single processing steps were implemented as dedicated commands.
An analysis run was a pipeline of multiple commands executed one after the other, in which the output of the previous command served as input for the next one.
This strict serialization of processing steps limited the overall performance that could be achieved.
Therefore, the ORT server not only aims to parallelize the single processing steps but also run them concurrently as far as possible.
For instance, as soon as a package has been detected by the _analyzer_, it can be picked up and processed by both the _advisor_ and the _scanner_.
These are isolated jobs, and there is no point in waiting until all other packages have been identified.

This approach of interleaving processing steps makes it of course harder to keep an overview over the progress of a specific analysis run.
While in the classic model a run was completed after the final step was done, now multiple stages can run a parallel.
It could even be the case that a later processing stage finishes its (partial) work while an earlier stage is still ongoing.
It is in the responsibility of the orchestrator to keep track on the progress of analysis runs, schedule new jobs when (temporary) results become available, and determine when a run is complete.
To achieve this, it has to store some information in addition to the <<ORT result>> tables discussed earlier.
These additional data structures serve multiple purposes:

* It must be possible to detect when all the jobs of an analysis run are done, so that the whole run is completed.
* Especially for monitoring purposes, progress information is of interest not only on a coarse but also on a finer level, e.g. how long did which processing step take, what was the average processing time of packages, etc.
* For a UI it is helpful to have detailed information to visualize the progress of a run.
While it is probably hard to give reliable estimations about the remaining processing time or the progress in percent, it should be possible to display statistics about the number of jobs that are pending or have been completed so far.
* In case of a server shutdown (expected or unexpected), it must be possible to resume interrupted analysis runs and continue with the work where it has been interrupted.

NOTE: In a production environment, care should be taken to keep server downtimes to a minimum.
With multiple redundant service instances running on a Kubernetes cluster, rolling updates, blue/green deployments, etc., this should indeed be possible with reasonable effort.
So the last point hopefully becomes less relevant; nevertheless, one needs to be prepared for the unexpected.

The following diagram shows the entities used by the orchestrator component.
Since their meaning is not that obvious, some more details are given for each of them below:

[plantuml,orchestrator,svg]
----
include::db_orchestrator.puml[]
----

[cols="1,3",options=header]
|===
|Entity|Description
|PACKAGE_PROGRESS
|This table holds information about the processing steps that have been applied to the single packages found in an analysis run. A row references the run it belongs to and the package affected. It contains the start time, i.e. the time when the package was detected by the analyser. If an end time is present, this means that this package has been fully processed.
|PACKAGE_PROGRESS_STEP
|On each package detected by the analyser, multiple processing steps need to be executed. In some cases, the order of these steps is relevant - if a step needs the results of a previous one -, in others not. Also, it may be the case that the steps to apply on a package are variable (e.g. if the user wants to run only specific steps) or (as a future extension) some packages may require special steps. To deal with such cases in a flexible manner, there is a 1-to-many relation between `PACKAGE_PROGRESS` and `PACKAGE_PROGRESS_STEP`; the number of steps for a specific run may vary. With the information stored in this table, it can be tracked which steps have already been applied to this package. Based on this information, the orchestrator can decide, what else needs to be done with this package.

It is up to a specific processing service to populate this table and therefore indicate when this special step is done. For instance, during the _advisor_ step multiple concrete advisors may be invoked for a package. By creating and storing advisor results, the component knows when all information for this package is available and can mark this step as completed.
|PROJECT_PROGRESS
|This table is analogous for `PACKAGE_PROGRESS`, but focuses on the projects discovered during an analysis run. On projects a number of processing steps needs to be applied as well, but not necessarily the same as for packages. For instance, the source code of projects can be scanned, but the project coordinates will not be sent to a security vulnerability database.
|PROJECT_PROGRESS_STEP
|The corresponding structure with processing steps for projects.
|RUN_PROGRESS
|Via this table, the progress of a whole analysis run can be tracked. It references the `ORT_RUN` it belongs to and stores the start and end time. If an end time is available, the run is considered to be complete.

In addition, the table stores an end time of the _analyzer_ step. The background is, as long as this property is undefined - and the _analyzer_ is running -, the run cannot be considered finished, since more packages may be discovered. This is true even if all packages detected so far have already been fully processed.
|===
