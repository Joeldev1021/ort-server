= Database schema (high-level design)

This document introduces the database schema of the ORT server. Since the data to be stored is rather complex, the whole schema is broken down into multiple logical areas. For each area, there are one or more diagrams showing the entities and their relations.

Note that the diagrams do not model every single details. The intended goal is rather to have an overview over the different entities, their most important attributes, and how they are connected.

== General design
This section discusses some general design topics and how they are dealt with in the ORT server schema.

=== Synthetic primary keys
The debate whether to prefer natural or synthetic (artificial/surrogate) primary keys is still ongoing. See for instance https://sqlstudies.com/2016/08/29/natural-vs-artificial-primary-keys/ or https://medium.com/swlh/relational-databases-know-your-primary-keys-3897befe9d2. This design uses synthetic keys exclusively for the following reasons:

* For many entities - especially in the ORT result model - there are no obvious natural keys.
* There are many relations between tables. Basing these on synthetic keys is easier and more efficient than using natural keys for this purpose.

=== "Shared" versus "redundant" tables
In typical relational database modeling, a goal is to be free of redundancy as far as possible. This means that tables define _UNIQUE_ constraints on identifying properties, and a specific instance of an entity exists only once. This has obvious advantages. For instance, the size of the database is reduced as it is free of redundancy, or statistics about entities are easier to calculate because no de-duplication has to be implemented. Therefore, this approach is certainly preferred. In some constellations, however, there are drawbacks:

* Adding new entities becomes harder. It has to be checked first whether a specific entity already exists in the table; if so, the ID of this entity has to be retrieved, otherwise, a new row is added to the table. Postgres offers an https://www.postgresqltutorial.com/postgresql-tutorial/postgresql-upsert/[INSERT ON CONFLICT] or _UPSERT_ statement for this purpose. However, its usage is not trivial.
* For entities with many attributes it is not always easy to find out whether a specific instance already exists. This is especially true for `Package` and `Project`: For these entities, identity is usually controlled by the components of an ORT `Identifier` (type, namespace, name, and version). However, for two entities with the same identifier, it cannot be guaranteed that all other properties match. For instance, after adding new curations, metadata of a package may change compared to an instance from an older analysis run. Detecting such cases and preventing redundancy correctly would mean a high effort.

So, choosing one of these approaches exclusively, seems to be too strict; a choice can be made based on the entity type. For entities with a limited number of attributes (which mainly remain static over time), instances should be de-duplicated and shared. Examples for this category are (source code) repositories, SPDX license expressions, or author names. In other cases, as for the mentioned projects and packages - but also for issues, vulnerabilities, or environment information -, new instances can be created for every analysis run.

=== JSON columns
The database schema defined here tries to be rather normalized. For some cases, an exception was made though, and ORT structures are stored in their serialized form in JSON columns. This is done mainly for configuration information for the different ORT components, which has the following properties:

* The structures are quite complex consisting of multiple subcomponents including arbitrary key-value pairs.
* The data is mainly stored for reference purpose. It is not further evaluated during an ORT analysis run.

So, there is no actual benefit in taking the effort to create a relational model for these structures.

== Data model
This section discusses the single areas of the data model of the ORT server. Each area has its own subsection which - depending on its complexity - may be further broken down.

=== ORT result
In the CLI version of ORT, YAML files are used to store the results of the single processing steps. This section describes a number of entities that are capable to store equivalent information. ORT result files have a hierarchical structure, sometimes containing lists of objects within other lists. This design does not fit that well to relational database structures; therefore, numerous foreign-key relations and association tables are required to store and correctly link the corresponding data. This makes this part of the database schema quite complicated. To keep the size of the diagrams manageable, multiple diagrams are created that focus on the single substructures (such as analyzer, scanner, advisor) existing in an ORT result file.

==== Global and shared entities
This diagram contains the entities that do not or not exclusively belong to a specific ORT component.

[plantuml, result_shared, svg]
----
include::db_result_shared.puml[]
----

`ORT_RUN` represents the result of an ORT analysis as a whole and thus corresponds to a single result file. It can be seen as the entrypoint into the model: starting from here, all information related to an analysis of a repository can be obtained.

The entities `ORT_ISSUE`, `LICENSE_SPDX`, `ENVIRONENT`, and `VCS_REPOSITORY` are used by multiple other components.

==== Analyzer
The analyzer section of an ORT result lists the projects that have been analyzed (i.e. the actual source code) and the external packages they depend on. This includes the whole dependency graph.

[plantuml, analyzer, svg]
----
include::db_analyzer.puml[]
----

This part of the data model is by far the most complex one. Entry point is the `ANALYZER_RESULT` entity, which combines the properties of the ORT model classes `AnalyzerRun` and `AnalyzerResult`. It is associated with the tables for projects and curated packages. The dependency graph is modelled in a way similar to the structure in an ORT result file; as lists of nodes and edges, and entry points into the graph (for the direct dependencies of project scopes). In contrast to ORT result files, no numeric indices are used here, but references between entities are expressed via foreign key relations. (Note that since the dependency graph can contain both packages and projects, these relations can have both tables as targets.)

==== Advisor
The advisor part adds information about vulnerabilities and defects to an ORT result.

[plantuml, advisor, svg]
----
include::db_advisor.puml[]
----

==== Scanner
This part of the model defines license and copyright findings provided by external scanner tools. Corresponding to the state of the ORT data model, there is no dedicated modelling for snipped scanners; this may be extended in the future.

[plantuml, scanner, svg]
----
include::db_scanner.puml[]
----

==== Evaluator
The last part of an ORT result to be stored in the database is the evaluator result. Compared to the other parts, this is a quite simple model, since it only adds a set of rule violations.


[plantuml, evaluator, svg]
----
include::db_evaluator.puml[]
----

=== Customer model
This part of the data model deals with information about customers and their repositories they want to analyze with ORT. This information is organized in a hierarchical model:

On the highest level, there are _organizations_. Organizations develop _products_. Products in turn can have multiple _repositories_ containing the actual source code to be analyzed. On each level, users with different roles can be assigned.

==== Association with Keycloak
There is obviously a strong connection between the entities in this part of the model and the role and access management to be implemented in the server. For each level of the hierarchy, special roles must exist:

* An admin of an organization can
** add or remove users to or from this organization
** assign roles to users in this organization
** create, update, or delete products in this organization
** create, update, or delete repositories in the products belonging to this organization
* An admin of a product can
** add or remove users to or from this product
** assign repository roles to users in this product
** create, update, or delete repositories in the products belonging to this product
* An admin of a repository can
** trigger an analysis run on a repository
** assign repository roles to users in this repository
* A user assigned to a repository can
** read the analysis results generated for this repository

For each request sent by a user, the server has to check whether the user has corresponding access rights. The most convenient way to do this is if the access token provided in the request already contains sufficient information required for this check: the organizations, products, and repositories assigned to the user and all the roles he or she is given. So, this implies that this information is available in Keycloak.

The data structures supported by Keycloak are actually capable to represent the customer data model: Organizations and products can be mapped to groups (which can be hierarchical), and access rights on repositories can be represented by roles (this approach is currently used for the mini server). The predefined properties of these structures in Keycloak are quite limited; it is, however, possible to assign arbitrary key-value pairs - so called _attributes_ to them. So it would be an option to store this data directly in Keycloak. Updates can be done via the Admin REST API. If these API calls are hidden behind a repository implementation, for the client code it is fully transparent that the data comes from a different source.

Based on these thoughts, a diagram for the entities in this area could look as follows:

[plantuml, customer, svg]
----
include::db_customer.puml[]
----
