= Kubernetes Job Monitor Component

This module is an add-on to the link:../kubernetes/README.adoc[Kubernetes Transport] implementation. It implements
robust job handling and cleanup of completed jobs.

== Synopsis
Workers spawned by the Orchestrator report their status - success or failure - on completion by sending a corresponding
message back to the Orchestrator. That way the Orchestrator can keep track on an ongoing ORT run and trigger the next
steps to make progress.

In a distributed setup, however, there is always a chance that a worker job crashes completely before it can even send
a failure message. In that scenario, without any further means, the Orchestrator would not be aware of the (abnormal)
termination of the job; thus the whole run would stall.

The purpose of this component is to prevent this by implementing an independent mechanism to detect failed jobs and
sending corresponding notifications to the Orchestrator. With this in place, it is guaranteed that the Orchestrator is
always notified about the outcome of a job it has triggered.

== Functionality
For the detection of failed jobs, the Job Monitor component actually implements two strategies:

* It uses the https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes[Kubernetes Watch API]
  to receive notifications about changes in the current state of jobs. Based on such change events, it can detect
  failed jobs and act accordingly.
* In addition, it lists the currently active jobs periodically and inspects this list for failed jobs. This is done for
  the following reasons:
  ** The scanning of jobs in regular intervals is a safety net in case a relevant change event was missed by the
    watching part. This could happen for instance if the monitor component was shortly down or gets restarted. It is
    then still guaranteed that the Orchestrator eventually receives a notification.
  ** Based on the job list, it is possible to remove completed jobs and their associated pods. This is not done
    out-of-the-box by Kubernetes; so the set of completed jobs would permanently grow. Therefore, the monitor component
    does an automatic cleanup of older jobs.

== Configuration
Some aspects of the component can be configured in the module's configuration file or via environment variables. The
fragment below shows the available configuration options:

.Configuration example
[source]
----
jobMonitor {
  namespace = "ortserver"
  enableWatching = true
  enableReaper = true
  reaperInterval = 600
}
----

The properties have the following meaning:

.Configuration options
[cols="1,1,3",options="header"]
|===
| Property | Variable | Description

| namespace
| MONITOR_NAMESPACE
| Defines the namespace in which jobs are to be monitored. This is typically the same namespace this component is
deployed in.

| enableWatching
| MONITOR_WATCHING_ENABLED
| A flag that controls whether the watching mechanism is enabled. If set to *false*, the component will not register
itself as a watcher for job changes. This can be useful for instance in a test environment where failed jobs should not
be cleaned up immediately.

| enableReaper
| MONITOR_REAPER_ENABLED
| A flag that controls whether the part that scans for completed and failed jobs periodically (aka the _Reaper_) is
active. Again, it can be useful to disable this part to diagnose problems with failed jobs.

| reaperInterval
| MONITOR_REAPER_INTERVAL
| The interval in which the periodic scans for completed and failed jobs are done (in seconds). This can be used to
fine-tune the time completed jobs are kept.
|===

In addition to these options, the configuration must contain a section defining the link:../README.adoc[transport]
for sending notifications to the Orchestrator.

== Open Points
In the current state of the implementation, the information the Job Monitor component sends to the Orchestrator when it
detects a failed job is limited. This is due to the fact that from a failed job not much information about the
associated ORT task is available. Especially, there is no way currently to determine the job ID or the ORT run ID,
which would both be particularly helpful for the Orchestrator. Instead, messages sent to the Orchestrator only contain
the name of the worker endpoint (analyzer, advisor, ...) and the trace ID, which can both be obtained from labels
assigned to the job.

There are multiple solutions to deal with this shortcoming:

* The Orchestrator could store the trace ID together with the ORT run in the database. It is then able to map it back
  to the affected run. This could be useful in general, since it would simplify log analysis for specific ORT runs.
* The trace ID could be derived somehow from the ORT run ID - after all, it just has to be unique. So it would be
  possible to compute the ORT run ID again based on the trace ID.
* Since all worker jobs are passed an ID parameter, it could be an option to expose this ID in a special way in the
  corresponding messages, e.g. by defining an interface to be implemented or by using a special message header field.
  Then the Kubernetes transport implementation would be able to extract this ID and store it in a specific label,
  which can be queried again by the Job Monitor component.

Other than that, the implementation should be production-ready.
